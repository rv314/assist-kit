# AssistKit ğŸ§ 

**Modular GenAI Assistant Framework**  
An extensible, pluggable framework to build powerful, context-aware assistants for multiple domains and tasks â€” including Chat, RAG (Retrieval-Augmented Generation), and more.

---

## ğŸš€ Overview

**AssistKit** is designed as a modular backend assistant engine that powers intelligent interactions using LLMs and embeddings.  
Built with extensibility in mind, it supports multiple use cases:

- ğŸ’¬ **Chat Assistant** with memory (chat_logs)
- ğŸ“„ **RAG Pipeline** for PDF, website, or custom document Q&A
- ğŸ§© Plugin-style architecture for future tools like Web Search, SQL Agent, etc.
- ğŸ”— Pluggable registries for LLMs, Embedders, and Vector DBs
- ğŸ–¥ï¸ **NiceGUI Frontend**: Clean UI layer for interacting with AssistKit via web browser (Planning to switch to Next.js or other framework in future)

---

## ğŸ”§ Current Modules

| Module          | Description                                              |
|-----------------|----------------------------------------------------------|
| `api/chat.py`   | Handles user chat, stores context in `chat_logs` vector  |
| `orchestrator.py` | Coordinates model interaction and context retrieval |
| `session_manager.py` | Handles chat sessions (JSON for now, pluggable)     |
| `vector_store.py` | Stores and retrieves documents from Vector DB          |
| `prompt_loader.py` | Loads system prompts dynamically from templates       |

---

## ğŸ§  Architecture
```bash
User â†’ FastAPI (api/chat or api/rag)
â†’ Orchestrator (InteractionEngine)
â”œâ”€ get_llm() from llm_registry
â”œâ”€ get_embedder() from embedding_registry
â”œâ”€ get_vector_db() from vector_registry
â””â”€ vector_store â†’ stores/queries via embeddings
```
- **ChatEngine**: Current base class in orchestrator; will be subclassed by `RAGEngine`, `WebSearchEngine`, etc.
- **Embeddings & LLMs**: Abstracted behind registries for easy swapping
- **Collections**: Support for multiple vector DB collections (e.g., `chat_logs`, `rag_docs`)

---

## ğŸ“ Folder Structure (WIP)
```bash
|-- api
|   `-- chat.py
|-- core
|   |-- __init__.py
|   |-- agents
|   |-- config
|   |-- embedding_models
|   |-- engine
|   |-- guardrails
|   |-- llm_providers
|   |-- prompts
|   |-- rag
|   |-- registries
|   |-- session_backends
|   |-- utils
|   |-- vector_backends
|-- documents
|   |-- examples
|   |-- processed
|   |-- uploads
|-- main.py
|-- session_data
|   |-- session_data.json
|-- tests
|   |-- __init__.py
|   |-- test_chat_client.py
|   |-- test_token_utils.py
|   |-- test_vectorstore.py
|-- ui
|   |-- __init__.py
|   |-- components
|   |-- main.py
|   |-- pages
|-- vectors
    |-- chroma
```

---

## ğŸ”œ Roadmap

- âœ… RAG Engine using uploaded files or web-scraped content
- ğŸ—‚ Pluggable storage backends (JSON â†’ Redis/PostgreSQL)
- ğŸ§¾ PDF and Web QA pipelines
- ğŸ” Web Search Agent / Other Agents / Agent Design Patterns
- Model Context Protocol (MCP)
- â˜ï¸ Deployable Dockerfile and Installation Guide

## Research (Planning to include) 

- Knowledge Graph (Neo4j or other Open Source)

## Use Cases

Idea to build this framework is to re-utilize it for various purposes / use-cases. Currently focusing on building the foundations.

---

## ğŸ“Œ Notes

- The current focus is on **local development**, so installation and deployment instructions will be added soon.
- LLM and Embedding provider is **OpenAI** and vector DB is **Chroma** (pluggable).
- Sessions are currently stored in JSON files but are designed to switch backends easily.

---

## ğŸ§‘â€ğŸ’» Contributing

Feature suggestions, refactors, or bug reports are welcome.  
To contribute:

1. Fork the repo
2. Create a feature branch
3. Submit a pull request with your changes and description

---

## ğŸ“„ License

MIT License

---
